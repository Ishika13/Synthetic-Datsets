{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebHdikiikkzF"
   },
   "outputs": [],
   "source": [
    "from rich import print\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OUx2RFlqlU5M",
    "outputId": "065fb7bf-ee9d-43ed-ab87-806e77737b77"
   },
   "outputs": [],
   "source": [
    "# Importing\n",
    "\n",
    "!pip install openai\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CWFy1GM6leJC",
    "outputId": "eb2891b6-ee1e-4b52-8f51-d32bf88ebad2"
   },
   "outputs": [],
   "source": [
    "# Setting up environment\n",
    "\n",
    "!pip install python-dotenv\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ei25NOIzlthK",
    "outputId": "ae3bb6c8-323b-4a74-e4af-0051332937c0"
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "\n",
    "!pip install datasets\n",
    "from datasets import Dataset, DatasetDict, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "_BgcLV6utf4n",
    "outputId": "5ec6d634-2298-4b18-80c6-83e0cf3e677a"
   },
   "outputs": [],
   "source": [
    "# Define and generate subtopics\n",
    "\n",
    "subtopics = 5\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\\\n",
    "Create a synthetic dataset of natural language and Git commands. Give me {subtopics} subtopics\n",
    "to cover what needs to be covered while working with Git.\n",
    "The list must be without numbers, and without any description of the subtopics. \n",
    "The subtopics must be separated by a comma. There must be no other text than the list.\n",
    "\"\"\"\n",
    "\n",
    "def generate_subtopics(client, subtopics):\n",
    "    prompt = PROMPT_TEMPLATE.format(subtopics=subtopics)\n",
    "    response = client.chat.completions.create(\n",
    "        model = MODEL,\n",
    "        messages = [\n",
    "            { \"role\": \"user\",\n",
    "             \"content\": prompt }\n",
    "        ],\n",
    "        temperature = 0.2,\n",
    "        top_p = 0.7,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "responses = generate_subtopics(client, subtopics = subtopics)\n",
    "print(responses.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E9gZzvpgu24H"
   },
   "outputs": [],
   "source": [
    "# Define and generate instructions\n",
    "\n",
    "instructions = 100\n",
    "\n",
    "INSTRUCTION_PROMPT_TEMPLATE = \"\"\"\\\n",
    "The objective is to create a dataset of user instructions in natural language that should be returned by Git commands.\n",
    "Given a topic in Git, generate {instructions} possible concise instructions that could be given to an AI assitant about that topic.\n",
    "Write some of these instructions as if given by someone with limited knowledge of Git terminologies and knowledge,\n",
    "like a beginner programmer. Your response should be in a list format.\n",
    "\n",
    "The topic is: {sub_topic}\n",
    "The list must be without numbers. The questions/instructions should be separated by a newline character. There must be no other text than the list.\n",
    "\"\"\"\n",
    "\n",
    "subtopic_list = responses.choices[0].message.content.split(\",\")\n",
    "def generate_instructions(client, sub_topic, instructions):\n",
    "    print(f\"Generating Instructions for {sub_topic}.\")\n",
    "    prompt = INSTRUCTION_PROMPT_TEMPLATE.format(sub_topic=sub_topic, instructions=instructions)\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages = [\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        top_p=0.7,\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 991
    },
    "id": "YlyYKty4Sgmc",
    "outputId": "241c10d3-3de0-4bba-a3fd-75bb2fd074a8"
   },
   "outputs": [],
   "source": [
    "def instructions_generator(client, subtopic_list, instructions):\n",
    "    instruction_list = [generate_instructions(client, subtopic, instructions) for subtopic in subtopic_list]\n",
    "    return instruction_list\n",
    "\n",
    "instruction_list = instructions_generator(client, subtopic_list, instructions)\n",
    "\n",
    "instruction_list_formatted = []\n",
    "for instruction_set in instruction_list:\n",
    "    instruction_list_formatted.extend([instruction.strip() for instruction in instruction_set.split(\"\\n\") if instruction])\n",
    "print(instruction_list_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jWoJPQoHu-lO"
   },
   "outputs": [],
   "source": [
    "# Define response template\n",
    "\n",
    "RESPONSE_PROMPT_TEMPLATE = \"\"\"\\\n",
    "Given an question/instruction related to Git, generate a response that could be given.\n",
    "Keep the response on-topic, informative, concise.\n",
    "\n",
    "The user prompt is: {instruction}\n",
    "\"\"\"\n",
    "def generate_responses(client, instruction):\n",
    "    prompt = RESPONSE_PROMPT_TEMPLATE.format(instruction=instruction)\n",
    "    response = client.chat.completions.create(\n",
    "        model = MODEL,\n",
    "        messages = [\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": prompt}\n",
    "        ],\n",
    "        temperature = 0.2,\n",
    "        top_p = 0.7,\n",
    "        max_tokens = 1024,\n",
    "    )\n",
    "    if isinstance(response, str):\n",
    "        print(f\"API returned a string: {response}\")\n",
    "        # Error handling\n",
    "        return \"Error: API returned a string.\"\n",
    "    else:\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lLMEsUsV5nDc",
    "outputId": "394e0fff-9362-4c53-80aa-fcc8b6364d9c"
   },
   "outputs": [],
   "source": [
    "# Generate response\n",
    "\n",
    "def response_generator(client, instruction_list):\n",
    "    response_list = [generate_responses(client, instruction) for instruction in instruction_list]\n",
    "    return response_list\n",
    "\n",
    "instruction_response_list = []\n",
    "instruction_response_list = response_generator(client, instruction_list_formatted)\n",
    "print(instruction_response_list)\n",
    "instruction_response_pair_list = []\n",
    "for instruction, response in zip(instruction_list_formatted, instruction_response_list):\n",
    "    instruction_response_pair_list.append(\n",
    "        {\n",
    "            \"instruction\": instruction,\n",
    "            \"responses\": response,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(instruction_response_pair_list)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
